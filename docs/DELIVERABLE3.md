# Παραδοτέο 3

Στο τρίτο σκέλος της εργασίας παραλληλοποιήσαμε το implementation και προσπαθήσαμε να μεγιστοποιήσουμε το throughput.
Οι βασικοί άξονες που διαλέξαμε να επεκτείνουμε ήταν το API usability και το throughput. Αυτό είχε ως αποτέλεσμα να
καταγράψουμε μεγαλύτερη κατανάλωση μνήμης.

## Job Scheduler

Η υλοποίηση του job scheduler πήρε έμπνευση απο συστήματα με actors. Η υλοποίηση βασίζεται σε ένα αρχικό threadpool απο
workers οι οποίοι έχουν ένα ξεχωριστό queue απο jobs ο καθένας στα οποία το scheduler βάζει jobs χρησιμοποιώντας έναν
round robin μηχανισμό. Η επιρροή των actors βρίσκεται στον τρόπο με τον οποίο υλοποιούνται τα jobs. Ένα job δεν είναι
τίποτα άλλο πέρα απο ένα struct που κάνει override το function call operator του abstract job απο το οποίο και κάνει
inherit. Αυτό επιτρέπει στον job scheduler να είναι αγνωστικός ως προς το είδος της δουλειάς που θα κάνει schedule αλλά
ταυτόχρονα δίνει στον χρήστη του API ένα ξεκάθαρο interface για να δημιουργήσει και να κάνει schedule type safe jobs.
Αυτό έχει βέβαια σαν αποτέλεσμα περισσότερα heap allocations και λόγο του βαθμού παραλληλίας μεγαλύτερη κατανάλωση μνήμης.
Για την υλοποίηση του `wait_all_finish` το scheduler αρκεί να περιμένει να αδιάσουν όλες οι ουρές με jobs του κάθε worker
αφού ο κάθε worker αφαιρεί το job απο την ουρά μόνο όταν τελειώσει το τρέξιμο του.

## Πειράματα που απέτυχαν

### Edit distance early stopping

Στην προσπάθεια μας να βελτιστοποιήσουμε το edit distance πέραν της εκμετάλευσης του cache locality δοκιμάσαμε να υλοποιήσουμε
έναν μηχανισμό early stopping. Αυτή η υλοποίηση αγνοούσε κάποια corner cases του bk tree με αποτέλεσμα να αποδίδει λάθος
αποτελέσματα υπο κάποιες συνθήκες.

### BK Tree edit distance caching

Με την χρήση profiler και flamegraph διαπιστώσαμε οτι το πρόγραμμα πέρναγε το περισσότερο CPU time του σε υπολογισμούς
edit distance. Με σκοπό να μειώσουμε τον αριθμό απο υπολογισμούς ήδη υπολογισμένων distances εισάγαμε ένα cache στο bk
tree το οποίο κράταγε το υπολογισμένο edit distance για unordered ζεύγη απο λέξεις. Αυτό είχε ως αποτέλεσμα τους λιγότερους
υπολογισμούς για edit distance που είχαν ήδη υπολογιστεί αλλά την αύξηση του συνολικού χρόνου εκτέλεσης. Αυτό ίσως να
οφείλεται στην μορφή ή των αριθμό των δεδομένων.


## ΒΕΛΤΙΣΤΟΠΟΙΗΣΕΙΣ ΠΟΥ ΥΛΟΠΟΙΗΘΗΚΑΝ

### Παράλληλο match

Η τελική υλοποίηση μας εκτελεί παράλληλα το `MatchDocument` function ώς εξής:

- Το `MatchDocument` γίνεται submit σαν job στον scheduler
- Ξεκινάει ένα καινούργιο scheduler με 3 queues
- Για κάθε είδος distance και για κάθε λέξη του document κάνει submit ένα job
- Το `MatchDocument` worker περιμένει μέσω του `wait_all_finish` να τελειώσουν οι δουλειές για να κάνει push τα τυχόν αποτελέσματα σε ένα queue απο το οποίο θα εξάγει αποτελέσματα το main thread μέσω της `GetNextAvailableRes`

Αυτό δίνει την δυνατότητα στο `MatchDocument` να επιστρέφει κατευθείαν στο main thread ενώ ένα worker thread αναλαμβάνει
να επιβλέψει το πραγματικό match. Παράλληλα το `GetNextAvailableRes` μπορεί να εξάγει αποτελέσματα με όποια σειρά έρθουν
χωρίς να χρειάζεται να περιμένει όλα τα match να τελειώσουν.


### Εκμετάλευση του cache locality για το edit distance

Ο αλγόριθμος δυναμικού προγραμματισμού του edit distance χρειάζεται ένα πίνακα 2 διαστάσεων για την υλοποίηση του.
Επιλέγοντας να το υλοποιήσουμε ως ένα buffer μιας διάστασης σε column major order καταφέραμε να έχουμε πολύ καλύτερο
cache locality με αποτέλεσμα την επιτάχυνση των edit distance.

### Iterative depth first traversal

Η πρώτη υλοποίηση του traversal του bk tree βασίζονταν σε αναδρομικό αλγόριθμο. Ενώ αυτό ήταν αρκετά γρήγορο για μικρό
όγκο δεδομένων όταν δοκιμάστηκε σε μεγαλύτερα data sets φάνηκε πως δεν θα ήταν αρκετό καθώς το call stack ξεπαιρνούσε
τα 1000 function σε βάθος. Με μια κλασσική iterative υλοποίηση με στοίβα καταφέραμε να μειώσουμε το συνολικό runtime
του traversal και να αποφύγουμε τυχόν stack overflow errors.


### Linear probing hash table

Η αρχική separate chaining υλοποίηση του hash table το οποίο χρησιμοποιήτε απο αρκετά σημεία δεν επαρκούσε για μεγαλύτερα
dataset. Για αυτό αλλάξαμε την υλοποίηση σε linear probing.

### String views

Για να αποφύγουμε extra allocations για τις λέξεις του document υλοποιήσαμε ένα `string_view` class το οποίο είναι ένα
slice πάνω απο ένα ήδη allocated string. Αυτό μας επέτρεψε να αυξήσουμε το throughput και να μειώσουμε το memory usage
ταυτόχρονα λόγο πολλών λιγότερων allocations.

## Πράγματα που θα θέλαμε να κάνουμε καλύτερα

### Job Groups

Η παράλληλη υλοποίηση του match ξεκινάει ένα καινούργιο schduler για κάθε document το οποίο έχει κόστος επειδή δεν μοιράζεται
τα thread του βασικού thread pool. Αυτό γίνεται γιατί αν τα μοιραζόταν το match job δεν θα μπορούσε να ξέρει πότε τελείωσαν
όλα τα subjobs τα οποία έκανε το ίδιο schedule. Αυτό το πρόβλημα θα μπορούσε να είχε λυθεί δημιουργόντας έναν μηχανισμό
για job groups για τα οποία θα μπορούσε να περιμένει μια ειδική υλοποίηση του `wait_all_finish`.

## Benchmarks

Αυτά είναι κάποια benchmarks σε διάφορα σημεία της παράλληλης υλοποίησης μας όλα καταγεγραμένα με το small_test.txt.
Δεν έχουν και πάρα πολυ συγκριτιή ισχύ βέβαια γιατί κάποιοα κομμάτια δεν είχαν υλοποιηθεί ακόμα σε εκείνο το σημείο
πχ. το παράλληλο match.

| Implementation                                  | Threads | Time (ms) | Throughput (docs/s) | Memory (MB) |
|-------------------------------------------------|---------|-----------|---------------------|-------------|
| HEAD                                            | 1       | 1440      | 666                 | 394         |
| HEAD                                            | 16      | 314       | 3057                | 393         |
| std::bind jobs (parallel insertion only)        | 1       | 1340      | 714                 | 503         |
| std::bind jobs (parallel insertion only)        | 16      | 1200      | 798                 | 503         |
| function pointer jobs (parallel insertion only) | 16      | 1012      | 948                 | 43          |
